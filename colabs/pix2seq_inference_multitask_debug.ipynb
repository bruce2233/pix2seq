{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#@title License\n",
        "# Copyright 2022 The Pix2Seq Authors.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# =============================================================================="
      ],
      "metadata": {
        "cellView": "form",
        "id": "0BkmPh5WJDHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A Unified Sequence Interface for Vision Tasks\n",
        "<a href=\"https://colab.research.google.com/github/google-research/pix2seq/blob/master/colabs/pix2seq_inference_multitask.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "\n",
        "This colab presents a demo for multi-task inference with Pix2seq. The table below provides a summary and model location for fine-tuned models on MSCOCO dataset.\n",
        "\n",
        "Backbone       | Total params (M) | Image size | COCO AP   | Google cloud storage location\n",
        "-------------: | ---------------: | ---------: | --------: | -----------:\n",
        "ViT-B          | 115.2            | 640x640    | 44.2      | [gs://pix2seq/multi_task/ckpt/vit_b_640x640](https://console.cloud.google.com/storage/browser/pix2seq/multi_task/ckpt/vit_b_640x640)\n",
        "ViT-B          | 115.2            | 1024x1024  | 46.5      | [gs://pix2seq/multi_task/ckpt/vit_b_1024x1024](https://console.cloud.google.com/storage/browser/pix2seq/multi_task/ckpt/vit_b_1024x1024)"
      ],
      "metadata": {
        "id": "iGuRf1kFH0-H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kEdYtgV-ZTF5"
      },
      "outputs": [],
      "source": [
        "#@title Imports.\n",
        "import os\n",
        "import sys\n",
        "\n",
        "!pip install tensorflow\n",
        "!pip install ml_collections\n",
        "!pip install tensorflow-addons\n",
        "!pip install tensorflow-text\n",
        "!git clone https://github.com/google-research/pix2seq.git\n",
        "sys.path.append(os.getcwd())\n",
        "root_dir = os.getcwd()\n",
        "sys.path.insert(1, 'pix2seq')\n",
        "\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import google.colab\n",
        "import ml_collections\n",
        "import json\n",
        "import copy\n",
        "import einops\n",
        "\n",
        "from models import ar_model as model_lib\n",
        "from tasks import instance_segmentation\n",
        "from tasks import keypoint_detection\n",
        "from tasks import object_detection\n",
        "from tasks import captioning\n",
        "from metrics import coco_metrics\n",
        "from tasks import task as task_lib\n",
        "from data import data_utils\n",
        "import utils\n",
        "from tasks.visualization import vis_utils\n",
        "from configs import config_multi_task"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops"
      ],
      "metadata": {
        "id": "cv3uZZG5dZZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Download coco annotations\n",
        "!mkdir /tmp/coco_annotations\n",
        "!wget https://storage.googleapis.com/pix2seq/multi_task/data/coco/json/captions_train2017_eval_compatible.json -P /tmp/coco_annotations/\n",
        "!wget https://storage.googleapis.com/pix2seq/multi_task/data/coco/json/captions_val2017_eval_compatible.json -P /tmp/coco_annotations/\n",
        "!wget https://storage.googleapis.com/pix2seq/multi_task/data/coco/json/instances_train2017.json -P /tmp/coco_annotations/\n",
        "!wget https://storage.googleapis.com/pix2seq/multi_task/data/coco/json/instances_val2017.json -P /tmp/coco_annotations/\n",
        "!wget https://storage.googleapis.com/pix2seq/multi_task/data/coco/json/person_keypoints_train2017.json -P /tmp/coco_annotations/\n",
        "!wget https://storage.googleapis.com/pix2seq/multi_task/data/coco/json/person_keypoints_val2017.json -P /tmp/coco_annotations/"
      ],
      "metadata": {
        "id": "MUY8cSCOVhb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRPSRPl49QV0"
      },
      "outputs": [],
      "source": [
        "#@title Load pix2seq multitask model.\n",
        "config = config_multi_task.get_config('object_detection@coco/2017_object_detection+instance_segmentation@coco/2017_instance_segmentation+keypoint_detection@coco/2017_keypoint_detection+captioning@coco/2017_captioning,vit-b')\n",
        "config.training = False\n",
        "\n",
        "# Restore checkpoint.\n",
        "model = model_lib.Model(config)\n",
        "checkpoint = tf.train.Checkpoint(\n",
        "    model=model, global_step=tf.Variable(0, dtype=tf.int64))\n",
        "model_dir = 'gs://pix2seq/multi_task/ckpt/vit_b_640x640'\n",
        "ckpt = tf.train.latest_checkpoint(model_dir)\n",
        "checkpoint.restore(ckpt).expect_partial()\n",
        "global_step = checkpoint.global_step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzK2JNQeAgBl"
      },
      "outputs": [],
      "source": [
        "#@title Get task and infer_fn.\n",
        "def get_task_config_and_infer_fn(config, task_name):\n",
        "  tconfig = copy.deepcopy(config)\n",
        "  for t in config.tasks:\n",
        "    if t['name'] == task_name:\n",
        "      tconfig.task = t\n",
        "  for d in config.datasets:\n",
        "    if d['name'] == task_name:\n",
        "      tconfig.dataset = d\n",
        "  tconfig.model_dir = ''\n",
        "\n",
        "  assert tconfig.task['name'] == task_name\n",
        "  task = task_lib.TaskRegistry.lookup(task_name)(tconfig)\n",
        "\n",
        "  if task_name == \"object_detection\":\n",
        "    tconfig.eval.batch_size = 1\n",
        "    tconfig.task.max_instances_per_image_test = 10\n",
        "  elif task_name == \"instance_segmentation\":\n",
        "    tconfig.task.use_gt_box_at_test = True\n",
        "    tconfig.task.ensemble_num_samples = 8\n",
        "    # For faster inference use smaller number of samples.\n",
        "    # tconfig.task.ensemble_num_samples = 1\n",
        "    tconfig.task.ensemble_threshold = 0.5\n",
        "    tconfig.eval.batch_size = 1\n",
        "    tconfig.task.max_instances_per_image_test = 1\n",
        "  elif task_name == \"keypoint_detection\":\n",
        "    tconfig.task.use_gt_box_at_test = True\n",
        "    tconfig.task.eval_suppress_invisible_token = True\n",
        "    tconfig.task.unbatch=True\n",
        "    tconfig.task.crop_to_bbox=True\n",
        "    tconfig.task.crop_to_bbox_pad_scale=0.5\n",
        "    tconfig.task.keypoint_score_weight=0.1\n",
        "    tconfig.task.points_score_weight = 0.1\n",
        "    tconfig.task.max_instances_per_image_test = 1\n",
        "    tconfig.eval.batch_size = 1\n",
        "    tconfig.task.top_p = 0.2\n",
        "  elif task_name == \"captioning\":\n",
        "    tconfig.task.captions_per_image = 1\n",
        "    tconfig.task.max_instances_per_image = 1\n",
        "\n",
        "  @tf.function\n",
        "  def infer(model, preprocessed_outputs):\n",
        "    return task.infer(model, preprocessed_outputs)\n",
        "\n",
        "  return tconfig, task, infer\n",
        "\n",
        "config_det, task_det, infer_det = get_task_config_and_infer_fn(config, \"object_detection\")\n",
        "config_seg, task_seg, infer_seg = get_task_config_and_infer_fn(config, \"instance_segmentation\")\n",
        "config_key, task_key, infer_key = get_task_config_and_infer_fn(config, \"keypoint_detection\")\n",
        "config_cap, task_cap, infer_cap = get_task_config_and_infer_fn(config, \"captioning\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4BCjazj20EWK"
      },
      "outputs": [],
      "source": [
        "#@title Functions to contruct inference examples and visualizations.\n",
        "\n",
        "def get_preprocessed_outputs_captioning(image, image_id, config):\n",
        "  im = image\n",
        "  ymin, xmin, ymax, xmax = bbox\n",
        "  h, w = np.shape(im)[0:2]\n",
        "\n",
        "  features = {\n",
        "      'image': tf.image.convert_image_dtype(np.array(im), tf.float32),\n",
        "      'image/id': image_id,\n",
        "      'orig_image_size': tf.shape(im)[0:2],\n",
        "  }\n",
        "  labels = {\n",
        "      'label': tf.zeros([1], tf.int64),\n",
        "      'bbox': tf.zeros([1, 4]),\n",
        "      'area': tf.zeros([1]),\n",
        "      'is_crowd': tf.zeros([1]),\n",
        "      # 'captions': tf.convert_to_tensor([\"dummy\"], tf.int64)\n",
        "  }\n",
        "\n",
        "  features, labels = data_utils.preprocess_eval(\n",
        "      features,\n",
        "      labels,\n",
        "      max_image_size=config.model.image_size,\n",
        "      max_instances_per_image=1)\n",
        "\n",
        "  # Batch features and labels.\n",
        "  features = {\n",
        "      k: tf.expand_dims(v, 0) for k, v in features.items()\n",
        "  }\n",
        "  labels = {\n",
        "      k: tf.expand_dims(v, 0) for k, v in labels.items()\n",
        "  }\n",
        "\n",
        "  preprocessed_outputs = (features['image'], None, (features, labels))\n",
        "  return preprocessed_outputs\n",
        "\n",
        "def print_captioning_result(outputs, tokenizer):\n",
        "  pred_seq = outputs[3]\n",
        "  pred_seq = tf.where(pred_seq == 0, 0, pred_seq - config_cap.model.text_vocab_shift)\n",
        "  print(tokenizer.detokenize([int(j) for j in pred_seq[0].numpy()]).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5W0tukYSBy1O"
      },
      "outputs": [],
      "source": [
        "# Download image.\n",
        "import requests\n",
        "def get_image(image_id, train=False):\n",
        "  image_id = \"{:0>6d}\".format(image_id)\n",
        "  split = 'train' if train else 'val'\n",
        "  url = f'http://images.cocodataset.org/{split}2017/000000{image_id}.jpg'\n",
        "  with tf.io.gfile.GFile(url) as f:\n",
        "    im = Image.open(requests.get(url, stream=True).raw)\n",
        "  return im"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VvwcagE9nMzD"
      },
      "outputs": [],
      "source": [
        "image_id = 230983\n",
        "im = get_image(image_id, train=False)\n",
        "im"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# You can also specify a custom box in (ymin, xmin, ymax, xmax) format.\n",
        "bbox = [148.5505782847104, 149.26714806698848, 325.032441173339, 294.49976727245297]"
      ],
      "metadata": {
        "id": "79myKp2-tp3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config_cap"
      ],
      "metadata": {
        "id": "FC95AaxangZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def shape_as_list(t):\n",
        "  # Assumes rank of `t` is statically known.\n",
        "  shape = t.shape.as_list()\n",
        "  print(shape)\n",
        "  dynamic_shape = tf.shape(t)\n",
        "  print(dynamic_shape)\n",
        "  return [\n",
        "      shape[i] if shape[i] is not None else dynamic_shape[i]\n",
        "      for i in range(len(shape))\n",
        "  ]\n",
        "\n",
        "def pad_to_max_len(data, max_len, dim, padding_token=0):\n",
        "  \"\"\"Pad the data tensor to max length on dim.\"\"\"\n",
        "  shape = shape_as_list(data)\n",
        "  padding_shape, new_shape = copy.copy(shape), copy.copy(shape)\n",
        "  padding_shape[dim] = max_len - padding_shape[dim]\n",
        "  new_shape[dim] = max_len\n",
        "  paddings = tf.fill(padding_shape, tf.cast(padding_token, dtype=data.dtype))\n",
        "  return tf.reshape(tf.concat([data, paddings], axis=dim), new_shape)\n",
        "\n",
        "caption = task_cap._tokenizer.string_to_ids(\"dummy\")\n",
        "print(caption)\n",
        "caption = pad_to_max_len(caption, 1, 0)"
      ],
      "metadata": {
        "id": "xP5RATayuMur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m39ybc2u68c_"
      },
      "outputs": [],
      "source": [
        "# Captioning.\n",
        "preprocessed_outputs = get_preprocessed_outputs_captioning(im, image_id, config_cap)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(preprocessed_outputs[2][1]) #label"
      ],
      "metadata": {
        "id": "6Pp7mL6jqwj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf.convert_to_tensor([\"dummy\"],tf.string))\n",
        "cap_str = tf.convert_to_tensor([\"dummy\"],tf.string)\n",
        "\n",
        "print(task_cap._tokenizer)\n",
        "tokens = task_cap._tokenizer.tokenizer.tokenize(cap_str)\n",
        "print(tokens)\n",
        "\n",
        "caption = task_cap._tokenizer.string_to_ids(tf.convert_to_tensor([\"dummy\"],tf.string))\n",
        "print(caption)"
      ],
      "metadata": {
        "id": "3qxgIBHdsPpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config_cap"
      ],
      "metadata": {
        "id": "mQRfCUT4rYDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "infer_outputs = infer_cap(model, preprocessed_outputs)"
      ],
      "metadata": {
        "id": "o21HCmkKotUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = task_cap.postprocess_tpu(*(infer_outputs[0][0]))\n",
        "print_captioning_result(outputs, task_cap._tokenizer.tokenizer)"
      ],
      "metadata": {
        "id": "WNhxkHBEXWDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print((infer_outputs[0][0]['captions']))"
      ],
      "metadata": {
        "id": "-C8Sueu9W9Ot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task_cap._tokenizer"
      ],
      "metadata": {
        "id": "O5P1NP7eVM80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd pix2seq/\n",
        "import os\n",
        "os.getcwd()"
      ],
      "metadata": {
        "id": "csZi0pIpeM-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_gan"
      ],
      "metadata": {
        "id": "cpHSK9Zlgc9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!config=configs/config_multi_task.py:captioning@coco/2017_captioning,vit-b\n",
        "!model_dir=/tmp/pix2seq_eval_cap\n",
        "\n",
        "!cd pix2seq && PYTHONPATH='./' python3 run.py --config='configs/config_multi_task.py:captioning@coco/2017_captioning,vit-b' --model_dir='/tmp/pix2seq_eval_cap' --mode=eval"
      ],
      "metadata": {
        "id": "XLfov0E4d-3h"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}